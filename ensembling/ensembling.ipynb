{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining different machine learning algorithms into an ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model ensembling** is a class of techniques for aggregating together multiple different predictive algorithm into a sort of mega-algorithm, which can often increase the accuracy and reduce the overfitting of your model. Ensembling approaches often work surprisingly well. Many winners of competitive data science competitions use model ensembling in [one](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf) [form](https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/5283/winning-solution-code-and-methodology) [or](http://arxiv.org/pdf/0911.0460.pdf) another. In this tutorial, we will take you through the steps of building your own ensemble of a random forest, support vector machine, and neural network for doing a classification problem. We’ll be working on the famous `spam` dataset and trying to predict whether a certain email is spam or not, and using the standard Python machine learning stack (`scikit`/`numpy`/`pandas`).\n",
    "\n",
    "You have probably already encountered several uses of model ensembling. **Random forests** are a type of ensemble algorithm that aggregates together many individual tree **base learners**. If you’re interested in **deep learning**, one common technique for improving classification accuracies is training different networks and getting them to vote on classifications for test instances (look at **dropout** for a related but wacky take on ensembling). If you’re familiar with **bagging** or **boosting** algorithms, these are very explicit examples of ensembling. \n",
    "\n",
    "Regardless of the specifics, the general idea behind ensembling is this: different classes of algorithms (or differently parameterized versions of the same type of algorithm) might be good at picking up on different signals in the dataset.  Combining them means that you can model the data better, leading to better predictions. Furthermore, different algorithms might be overfitting to the data in various ways, but by combining them, you can effectively average away some of this overfitting.\n",
    "\n",
    "We won’t do fancy visualizations of the dataset here.  Check out this tutorial or our bootcamp to learn Plotly and matplotlib. Here, we are focused on optimizing different algorithms and combining them to boost performance.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading up the data\n",
    "\n",
    "Load dataset. We often want our input data to be a matrix (X) and the vector of instance labels as a separate vector (y). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>is_spam</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_random</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>char_freq_(</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3628</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1540</td>\n",
       "      <td>no</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.176</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4460</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2771</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_id is_spam  word_freq_will  word_freq_original  word_freq_415  \\\n",
       "0      3628      no            0.00                   0              0   \n",
       "1        63      no            0.00                   0              0   \n",
       "2      1540      no            1.31                   0              0   \n",
       "3      4460     yes            0.75                   0              0   \n",
       "4      2771      no            0.00                   0              0   \n",
       "\n",
       "   word_freq_mail  char_freq_#  char_freq_$  word_freq_internet  \\\n",
       "0            0.00            0            0                 0.0   \n",
       "1            0.49            0            0                 0.0   \n",
       "2            0.00            0            0                 0.0   \n",
       "3            0.50            0            0                 0.5   \n",
       "4            0.00            0            0                 0.0   \n",
       "\n",
       "   word_freq_edu     ...       word_freq_receive  word_freq_000  \\\n",
       "0              0     ...                    0.00              0   \n",
       "1              0     ...                    0.00              0   \n",
       "2              0     ...                    0.00              0   \n",
       "3              0     ...                    0.25              0   \n",
       "4              0     ...                    0.00              0   \n",
       "\n",
       "   capital_run_length_average  word_freq_address  word_freq_george  \\\n",
       "0                       2.000               0.00              0.00   \n",
       "1                       2.824               0.00              0.99   \n",
       "2                       2.176               0.00              0.00   \n",
       "3                       1.023               0.75              0.00   \n",
       "4                       1.500               0.00              1.56   \n",
       "\n",
       "   word_freq_cs  word_freq_random  word_freq_conference  word_freq_technology  \\\n",
       "0             0                 0                     0                     0   \n",
       "1             0                 0                     0                     0   \n",
       "2             0                 0                     0                     0   \n",
       "3             0                 0                     0                     0   \n",
       "4             0                 0                     0                     0   \n",
       "\n",
       "   char_freq_(  \n",
       "0        0.000  \n",
       "1        0.062  \n",
       "2        0.431  \n",
       "3        0.180  \n",
       "4        0.180  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the dataset\n",
    "dataset_path = \"spam_dataset.csv\"\n",
    "dataset = pd.read_csv(dataset_path, sep=\",\")\n",
    "\n",
    "# Take a peak at the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning up and summarizing the data\n",
    "Lookin' good! Let's convert the data into a nice format. We rearrange some columns, check out what the columns are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 62)\n",
      "['word_freq_will' 'word_freq_original' 'word_freq_415' 'word_freq_mail'\n",
      " 'char_freq_#' 'char_freq_$' 'word_freq_internet' 'word_freq_edu'\n",
      " 'word_freq_hp' 'word_freq_lab']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>word_freq_hp</th>\n",
       "      <th>word_freq_lab</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_random</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>char_freq_(</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.537950</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>0.189840</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>0.066014</td>\n",
       "      <td>0.073210</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.611970</td>\n",
       "      <td>0.118610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051040</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>4.857610</td>\n",
       "      <td>0.149980</td>\n",
       "      <td>0.775740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036690</td>\n",
       "      <td>0.125580</td>\n",
       "      <td>0.144783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.831747</td>\n",
       "      <td>0.173041</td>\n",
       "      <td>0.365678</td>\n",
       "      <td>0.496022</td>\n",
       "      <td>0.109007</td>\n",
       "      <td>0.248239</td>\n",
       "      <td>0.270431</td>\n",
       "      <td>0.86285</td>\n",
       "      <td>1.734907</td>\n",
       "      <td>0.746169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192314</td>\n",
       "      <td>0.358906</td>\n",
       "      <td>30.226395</td>\n",
       "      <td>0.955315</td>\n",
       "      <td>3.509211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.268434</td>\n",
       "      <td>0.449092</td>\n",
       "      <td>0.232423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.541000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.219500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.396500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>4.017000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>20.830000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>2.941000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_will  word_freq_original  word_freq_415  word_freq_mail  \\\n",
       "count     1000.000000         1000.000000    1000.000000     1000.000000   \n",
       "mean         0.537950            0.038370       0.054690        0.189840   \n",
       "std          0.831747            0.173041       0.365678        0.496022   \n",
       "min          0.000000            0.000000       0.000000        0.000000   \n",
       "25%          0.000000            0.000000       0.000000        0.000000   \n",
       "50%          0.000000            0.000000       0.000000        0.000000   \n",
       "75%          0.820000            0.000000       0.000000        0.000000   \n",
       "max          6.250000            2.220000       4.760000        5.260000   \n",
       "\n",
       "       char_freq_#  char_freq_$  word_freq_internet  word_freq_edu  \\\n",
       "count  1000.000000  1000.000000         1000.000000     1000.00000   \n",
       "mean      0.022792     0.066014            0.073210        0.18100   \n",
       "std       0.109007     0.248239            0.270431        0.86285   \n",
       "min       0.000000     0.000000            0.000000        0.00000   \n",
       "25%       0.000000     0.000000            0.000000        0.00000   \n",
       "50%       0.000000     0.000000            0.000000        0.00000   \n",
       "75%       0.000000     0.016000            0.000000        0.00000   \n",
       "max       1.410000     4.017000            3.570000       10.00000   \n",
       "\n",
       "       word_freq_hp  word_freq_lab     ...       word_freq_receive  \\\n",
       "count   1000.000000    1000.000000     ...             1000.000000   \n",
       "mean       0.611970       0.118610     ...                0.051040   \n",
       "std        1.734907       0.746169     ...                0.192314   \n",
       "min        0.000000       0.000000     ...                0.000000   \n",
       "25%        0.000000       0.000000     ...                0.000000   \n",
       "50%        0.000000       0.000000     ...                0.000000   \n",
       "75%        0.315000       0.000000     ...                0.000000   \n",
       "max       20.830000      14.280000     ...                2.000000   \n",
       "\n",
       "       word_freq_000  capital_run_length_average  word_freq_address  \\\n",
       "count    1000.000000                 1000.000000        1000.000000   \n",
       "mean        0.081300                    4.857610           0.149980   \n",
       "std         0.358906                   30.226395           0.955315   \n",
       "min         0.000000                    1.000000           0.000000   \n",
       "25%         0.000000                    1.541000           0.000000   \n",
       "50%         0.000000                    2.219500           0.000000   \n",
       "75%         0.000000                    3.396500           0.000000   \n",
       "max         5.450000                  667.000000          14.280000   \n",
       "\n",
       "       word_freq_george  word_freq_cs  word_freq_random  word_freq_conference  \\\n",
       "count       1000.000000          1000              1000           1000.000000   \n",
       "mean           0.775740             0                 0              0.036690   \n",
       "std            3.509211             0                 0              0.268434   \n",
       "min            0.000000             0                 0              0.000000   \n",
       "25%            0.000000             0                 0              0.000000   \n",
       "50%            0.000000             0                 0              0.000000   \n",
       "75%            0.000000             0                 0              0.000000   \n",
       "max           33.330000             0                 0              5.000000   \n",
       "\n",
       "       word_freq_technology  char_freq_(  \n",
       "count           1000.000000  1000.000000  \n",
       "mean               0.125580     0.144783  \n",
       "std                0.449092     0.232423  \n",
       "min                0.000000     0.000000  \n",
       "25%                0.000000     0.000000  \n",
       "50%                0.000000     0.072000  \n",
       "75%                0.000000     0.195000  \n",
       "max                4.760000     2.941000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder the data columns and drop email_id\n",
    "cols = dataset.columns.tolist()\n",
    "cols = cols[2:] + [cols[1]]\n",
    "dataset = dataset[cols]\n",
    "\n",
    "# Examine shape of dataset and some column names\n",
    "print dataset.shape\n",
    "print dataset.columns.values[0:10]\n",
    "\n",
    "# Summarise feature values\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert dataframe to numpy array and split\n",
    "# data into input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-1].astype(float)\n",
    "y = npArray[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Splitting data into training and testing sets\n",
    "\n",
    "Our day is now nice and squeaky clean! This definitely always happens in real life. \n",
    "\n",
    "Next up, let's scale the data and split it into a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Scale and split dataset\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X_scaled, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running algorithms on the data\n",
    "\n",
    "Blah blah now it's time to train algorithms. We are doing binary classification. Could ahve also used logistic regression, kNN, etc etc.\n",
    "\n",
    "### 4.1 Random forests\n",
    "\n",
    "Let’s build a random forest. A great explanation of random forests can be found here. Briefly, random forests build a collection of classification trees, which each try to predict classes by recursively splitting the data on features that split classes best. Each tree is trained on bootstrapped data, and each split is only allowed to use certain variables. So, an element of randomness is introduced, a variety of different trees are built, and the 'random forest' ensembles together these base learners.\n",
    "\n",
    "A hyperparameter is something than influences the performance of your model, but isn't directly tuned during model training. The main hyperparameters to adjust for random forrests are `n_estimators` and `max_features`. `n_estimators` controls the number of trees in the forest - the more the better, but more trees comes at the expense of longer training time. `max_features` controls the size of the random selection of features the algorithm is allowed to consider when splitting a node. \n",
    "\n",
    "We could also choose to tune [various other hyperpramaters](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), like `max_depth` (the maximum depth of a tree, which controls how tall we grow our trees and influences overfitting) and the choice of the purity `criterion` (which are specific formulas for calculating how good or 'pure' our splits make the terminal nodes). \n",
    "\n",
    "We are doing gridsearch to find optimal hyperparameter values, which tries out each given value for each hyperparameter of interst and sees how well it performs using (in this case) 10-fold cross-validation (CV). As a reminder, in cross-validation we try to estimate the test-set performance for a model; in k-fold CV, the estimate is done by repeatedly partitioning the dataset into k parts and 'testing' on 1/kth of it. We could have also tuned our hyperparameters using randomized search, which samples some values from a distribution rather than trying out all given values. Either is probably [fine](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.95      0.98      0.96       170\n",
      "        yes       0.95      0.89      0.92        80\n",
      "\n",
      "avg / total       0.95      0.95      0.95       250\n",
      "\n",
      "('Overall Accuracy:', 0.95)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "n_estimators = np.arange(1, 30, 5)\n",
    "max_features  = np.arange(1, X.shape[1], 10)\n",
    "max_depth    = np.arange(1, 100, 10)\n",
    "\n",
    "parameters   = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth}\n",
    "\n",
    "# Grid search using cross-validation\n",
    "gridCV = GridSearchCV(RandomForestClassifier(), param_grid=parameters, cv=10, n_jobs=4)\n",
    "gridCV.fit(XTrain, yTrain)\n",
    "\n",
    "best_n_estim      = gridCV.best_params_['n_estimators']\n",
    "best_max_features = gridCV.best_params_['max_features']               \n",
    "best_max_depth    = gridCV.best_params_['max_depth']\n",
    "\n",
    "# Train classifier using optimal hyperparameter values\n",
    "clfRDF = RandomForestClassifier(n_estimators=best_n_estim, max_features=best_max_features, max_depth=best_max_depth)\n",
    "clfRDF.fit(XTrain, yTrain)\n",
    "predRF = clfRDF.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, predRF))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, predRF),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% accuracy, not too shabby! Have a look and see how random forests with suboptimal hyperparameters fare. We got around 92% accuracy on the out of the box (untuned) random forests, which actually isn't too bad. \n",
    "\n",
    "### 2) Second algorithm: support vector machines\n",
    "\n",
    "SVMs do X and Y. Different types are X, Y Z. The hyperparaeters for random forrests are X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.94      0.95      0.94       170\n",
      "        yes       0.88      0.86      0.87        80\n",
      "\n",
      "avg / total       0.92      0.92      0.92       250\n",
      "\n",
      "Overall Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "g_range = 2. ** np.arange(-15, 5, step=2)\n",
    "C_range = 2. ** np.arange(-5, 15, step=2)\n",
    "parameters = [{'gamma': g_range, 'C': C_range}] \n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(SVC(), parameters, cv= 10)  \n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "bestG = grid.best_params_['gamma']\n",
    "bestC = grid.best_params_['C']\n",
    "\n",
    "# Train SVM and output predictions\n",
    "rbfSVM = SVC(kernel='rbf', C=bestC, gamma=bestG)\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "yPredRBF = rbfSVM.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, yPredRBF)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, yPredRBF),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! \n",
    "\n",
    "### 3) Third algorithm: neural network\n",
    "\n",
    "Let's jump on the hype wagon and throw neural networks at our problem.\n",
    "\n",
    "NNs do X and Y. Different types are X, Y, Z. We optimize these hyperparams which control X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multilayer_perceptron import multilayer_perceptron\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "layer_size_range = [(3,2),(10,10),(2,2,2),10,5] # different networks shapes\n",
    "learning_rate_range = np.linspace(.1,1,3)\n",
    "parameters = [{'hidden_layer_sizes': layer_size_range, 'learning_rate_init': learning_rate_range}]\n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(multilayer_perceptron.MultilayerPerceptronClassifier(), parameters, cv=10)\n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "# Output best hyperparameter values\n",
    "best_size    = grid.best_params_['hidden_layer_sizes']\n",
    "best_best_lr = grid.best_params_['learning_rate_init']\n",
    "\n",
    "# Train neural network and output predictions\n",
    "nnet = multilayer_perceptron.MultilayerPerceptronClassifier(hidden_layer_sizes=best_size, learning_rate_init=best_best_lr)\n",
    "nnet.fit(XTrain, yTrain)\n",
    "net_prediction = nnet.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, net_prediction)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, net_prediction),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Majority vote on classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# majority voting code goes here lolll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Conclusion\n",
    "\n",
    "There are plenty of ways to do model ensembling. Simple majority voting. We can also do weighted majority voting, where models with higher accuracy get more of a vote. If your output is numerical, you could average. These relatively simple techniques do a great job, but there is more! Stacking (also called blending) is when the predictions from different algorithms are used as input into another algorithm (often good old linear and logistic regression) which then outputs your final predictions. For example, you might train a linear model on the predictions. Blending. \n",
    "\n",
    "\n",
    "\n",
    " It is best to ensemble together models which are less correlated (see an excellent explanation here). \n",
    "See an excellent explanation of ensembling here. \n",
    "\n",
    "\n",
    "What happens when your dataset isn’t as nice as this? What if there are many more instances of one class versus the other, or if you have a lot of missing values, or a mixture of categorical and numerical variables? Stay tuned for the next blog post where we write up guidance on tackling these types of sticky situations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "+ Should we use something cooler like gradient boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best to ensemble together models which are less correlated (see an excellent explanation [here](http://mlwave.com/kaggle-ensembling-guide/)). \n",
    "See an excellent explanation of ensembling [here](http://mlwave.com/kaggle-ensembling-guide/). \n",
    "\n",
    "Another nice tutorial on doing ensembling in python is [here](http://sebastianraschka.com/Articles/2014_ensemble_classifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
