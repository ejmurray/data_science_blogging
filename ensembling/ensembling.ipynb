{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining different machine learning algorithms into an ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model ensembling** is a class of techniques for aggregating together multiple different predictive algorithm into a sort of mega-algorithm, which can often increase the accuracy and reduce the overfitting of your model. Ensembling approaches often work surprisingly well. Many winners of competitive data science competitions use model ensembling in [one](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf) [form](https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/5283/winning-solution-code-and-methodology) [or](http://arxiv.org/pdf/0911.0460.pdf) another. In this tutorial, we will take you through the steps of building your own ensemble of a random forest, support vector machine, and neural network for doing a classification problem. We’ll be working on the famous `spam` dataset and trying to predict whether a certain email is spam or not, and using the standard Python machine learning stack (`scikit`/`numpy`/`pandas`).\n",
    "\n",
    "You have probably already encountered several uses of model ensembling. **Random forests** are a type of ensemble algorithm that aggregates together many individual tree **base learners**. If you’re interested in **deep learning**, one common technique for improving classification accuracies is training different networks and getting them to vote on classifications for test instances (look at **dropout** for a related but wacky take on ensembling). If you’re familiar with **bagging** or **boosting** algorithms, these are very explicit examples of ensembling. \n",
    "\n",
    "Regardless of the specifics, the general idea behind ensembling is this: different classes of algorithms (or differently parameterized versions of the same type of algorithm) might be good at picking up on different signals in the dataset.  Combining them means that you can model the data better, leading to better predictions. Furthermore, different algorithms might be overfitting to the data in various ways, but by combining them, you can effectively average away some of this overfitting.\n",
    "\n",
    "We won’t do fancy visualizations of the dataset here.  Check out this tutorial or our bootcamp to learn Plotly and matplotlib. Here, we are focused on optimizing different algorithms and combining them to boost performance.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading up the data\n",
    "\n",
    "Load dataset. We often want our input data to be a matrix (X) and the vector of instance labels as a separate vector (y). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>is_spam</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_random</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>char_freq_(</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3628</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1540</td>\n",
       "      <td>no</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.176</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4460</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2771</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_id is_spam  word_freq_will  word_freq_original  word_freq_415  \\\n",
       "0      3628      no            0.00                   0              0   \n",
       "1        63      no            0.00                   0              0   \n",
       "2      1540      no            1.31                   0              0   \n",
       "3      4460     yes            0.75                   0              0   \n",
       "4      2771      no            0.00                   0              0   \n",
       "\n",
       "   word_freq_mail  char_freq_#  char_freq_$  word_freq_internet  \\\n",
       "0            0.00            0            0                 0.0   \n",
       "1            0.49            0            0                 0.0   \n",
       "2            0.00            0            0                 0.0   \n",
       "3            0.50            0            0                 0.5   \n",
       "4            0.00            0            0                 0.0   \n",
       "\n",
       "   word_freq_edu     ...       word_freq_receive  word_freq_000  \\\n",
       "0              0     ...                    0.00              0   \n",
       "1              0     ...                    0.00              0   \n",
       "2              0     ...                    0.00              0   \n",
       "3              0     ...                    0.25              0   \n",
       "4              0     ...                    0.00              0   \n",
       "\n",
       "   capital_run_length_average  word_freq_address  word_freq_george  \\\n",
       "0                       2.000               0.00              0.00   \n",
       "1                       2.824               0.00              0.99   \n",
       "2                       2.176               0.00              0.00   \n",
       "3                       1.023               0.75              0.00   \n",
       "4                       1.500               0.00              1.56   \n",
       "\n",
       "   word_freq_cs  word_freq_random  word_freq_conference  word_freq_technology  \\\n",
       "0             0                 0                     0                     0   \n",
       "1             0                 0                     0                     0   \n",
       "2             0                 0                     0                     0   \n",
       "3             0                 0                     0                     0   \n",
       "4             0                 0                     0                     0   \n",
       "\n",
       "   char_freq_(  \n",
       "0        0.000  \n",
       "1        0.062  \n",
       "2        0.431  \n",
       "3        0.180  \n",
       "4        0.180  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the dataset\n",
    "dataset_path = \"spam_dataset.csv\"\n",
    "dataset = pd.read_csv(dataset_path, sep=\",\")\n",
    "\n",
    "# Take a peak at the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning up and summarizing the data\n",
    "Lookin' good! Let's convert the data into a nice format. We rearrange some columns, check out what the columns are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 62)\n",
      "['word_freq_will' 'word_freq_original' 'word_freq_415' 'word_freq_mail'\n",
      " 'char_freq_#' 'char_freq_$' 'word_freq_internet' 'word_freq_edu'\n",
      " 'word_freq_hp' 'word_freq_lab']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>word_freq_hp</th>\n",
       "      <th>word_freq_lab</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_random</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>char_freq_(</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.537950</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>0.189840</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>0.066014</td>\n",
       "      <td>0.073210</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.611970</td>\n",
       "      <td>0.118610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051040</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>4.857610</td>\n",
       "      <td>0.149980</td>\n",
       "      <td>0.775740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036690</td>\n",
       "      <td>0.125580</td>\n",
       "      <td>0.144783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.831747</td>\n",
       "      <td>0.173041</td>\n",
       "      <td>0.365678</td>\n",
       "      <td>0.496022</td>\n",
       "      <td>0.109007</td>\n",
       "      <td>0.248239</td>\n",
       "      <td>0.270431</td>\n",
       "      <td>0.86285</td>\n",
       "      <td>1.734907</td>\n",
       "      <td>0.746169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192314</td>\n",
       "      <td>0.358906</td>\n",
       "      <td>30.226395</td>\n",
       "      <td>0.955315</td>\n",
       "      <td>3.509211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.268434</td>\n",
       "      <td>0.449092</td>\n",
       "      <td>0.232423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.541000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.219500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.396500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>4.017000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>20.830000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>2.941000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_will  word_freq_original  word_freq_415  word_freq_mail  \\\n",
       "count     1000.000000         1000.000000    1000.000000     1000.000000   \n",
       "mean         0.537950            0.038370       0.054690        0.189840   \n",
       "std          0.831747            0.173041       0.365678        0.496022   \n",
       "min          0.000000            0.000000       0.000000        0.000000   \n",
       "25%          0.000000            0.000000       0.000000        0.000000   \n",
       "50%          0.000000            0.000000       0.000000        0.000000   \n",
       "75%          0.820000            0.000000       0.000000        0.000000   \n",
       "max          6.250000            2.220000       4.760000        5.260000   \n",
       "\n",
       "       char_freq_#  char_freq_$  word_freq_internet  word_freq_edu  \\\n",
       "count  1000.000000  1000.000000         1000.000000     1000.00000   \n",
       "mean      0.022792     0.066014            0.073210        0.18100   \n",
       "std       0.109007     0.248239            0.270431        0.86285   \n",
       "min       0.000000     0.000000            0.000000        0.00000   \n",
       "25%       0.000000     0.000000            0.000000        0.00000   \n",
       "50%       0.000000     0.000000            0.000000        0.00000   \n",
       "75%       0.000000     0.016000            0.000000        0.00000   \n",
       "max       1.410000     4.017000            3.570000       10.00000   \n",
       "\n",
       "       word_freq_hp  word_freq_lab     ...       word_freq_receive  \\\n",
       "count   1000.000000    1000.000000     ...             1000.000000   \n",
       "mean       0.611970       0.118610     ...                0.051040   \n",
       "std        1.734907       0.746169     ...                0.192314   \n",
       "min        0.000000       0.000000     ...                0.000000   \n",
       "25%        0.000000       0.000000     ...                0.000000   \n",
       "50%        0.000000       0.000000     ...                0.000000   \n",
       "75%        0.315000       0.000000     ...                0.000000   \n",
       "max       20.830000      14.280000     ...                2.000000   \n",
       "\n",
       "       word_freq_000  capital_run_length_average  word_freq_address  \\\n",
       "count    1000.000000                 1000.000000        1000.000000   \n",
       "mean        0.081300                    4.857610           0.149980   \n",
       "std         0.358906                   30.226395           0.955315   \n",
       "min         0.000000                    1.000000           0.000000   \n",
       "25%         0.000000                    1.541000           0.000000   \n",
       "50%         0.000000                    2.219500           0.000000   \n",
       "75%         0.000000                    3.396500           0.000000   \n",
       "max         5.450000                  667.000000          14.280000   \n",
       "\n",
       "       word_freq_george  word_freq_cs  word_freq_random  word_freq_conference  \\\n",
       "count       1000.000000          1000              1000           1000.000000   \n",
       "mean           0.775740             0                 0              0.036690   \n",
       "std            3.509211             0                 0              0.268434   \n",
       "min            0.000000             0                 0              0.000000   \n",
       "25%            0.000000             0                 0              0.000000   \n",
       "50%            0.000000             0                 0              0.000000   \n",
       "75%            0.000000             0                 0              0.000000   \n",
       "max           33.330000             0                 0              5.000000   \n",
       "\n",
       "       word_freq_technology  char_freq_(  \n",
       "count           1000.000000  1000.000000  \n",
       "mean               0.125580     0.144783  \n",
       "std                0.449092     0.232423  \n",
       "min                0.000000     0.000000  \n",
       "25%                0.000000     0.000000  \n",
       "50%                0.000000     0.072000  \n",
       "75%                0.000000     0.195000  \n",
       "max                4.760000     2.941000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder the data columns and drop email_id\n",
    "cols = dataset.columns.tolist()\n",
    "cols = cols[2:] + [cols[1]]\n",
    "dataset = dataset[cols]\n",
    "\n",
    "# Examine shape of dataset and some column names\n",
    "print dataset.shape\n",
    "print dataset.columns.values[0:10]\n",
    "\n",
    "# Summarise feature values\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert dataframe to numpy array and split\n",
    "# data into input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-1].astype(float)\n",
    "y = npArray[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Splitting data into training and testing sets\n",
    "\n",
    "Our day is now nice and squeaky clean! This definitely always happens in real life. \n",
    "\n",
    "Next up, let's scale the data and split it into a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Scale and split dataset\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X_scaled, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running algorithms on the data\n",
    "\n",
    "Blah blah now it's time to train algorithms. We are doing binary classification. Could ahve also used logistic regression, kNN, etc etc.\n",
    "\n",
    "### 4.1 Random forests\n",
    "\n",
    "Let’s build a random forest. A great explanation of random forests can be found here. Briefly, random forests build a collection of classification trees, which each try to predict classes by recursively splitting the data on features that split classes best. Each tree is trained on bootstrapped data, and each split is only allowed to use certain variables. So, an element of randomness is introduced, a variety of different trees are built, and the 'random forest' ensembles together these base learners.\n",
    "\n",
    "A hyperparameter is something than influences the performance of your model, but isn't directly tuned during model training. The main hyperparameters to adjust for random forrests are `n_estimators` and `max_features`. `n_estimators` controls the number of trees in the forest - the more the better, but more trees comes at the expense of longer training time. `max_features` controls the size of the random selection of features the algorithm is allowed to consider when splitting a node. \n",
    "\n",
    "We could also choose to tune [various other hyperpramaters](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), like `max_depth` (the maximum depth of a tree, which controls how tall we grow our trees and influences overfitting) and the choice of the purity `criterion` (which are specific formulas for calculating how good or 'pure' our splits make the terminal nodes). \n",
    "\n",
    "We are doing gridsearch to find optimal hyperparameter values, which tries out each given value for each hyperparameter of interst and sees how well it performs using (in this case) 10-fold cross-validation (CV). As a reminder, in cross-validation we try to estimate the test-set performance for a model; in k-fold CV, the estimate is done by repeatedly partitioning the dataset into k parts and 'testing' on 1/kth of it. We could have also tuned our hyperparameters using randomized search, which samples some values from a distribution rather than trying out all given values. Either is probably [fine](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html). \n",
    "\n",
    "The following code block takes about a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.94      0.96      0.95       170\n",
      "        yes       0.91      0.86      0.88        80\n",
      "\n",
      "avg / total       0.93      0.93      0.93       250\n",
      "\n",
      "('Overall Accuracy:', 0.93)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "n_estimators = np.arange(1, 30, 5)\n",
    "max_features  = np.arange(1, X.shape[1], 10)\n",
    "max_depth    = np.arange(1, 100, 10)\n",
    "\n",
    "hyperparameters   = {'n_estimators': n_estimators, \n",
    "                     'max_features': max_features, \n",
    "                     'max_depth': max_depth}\n",
    "\n",
    "# Grid search using cross-validation\n",
    "gridCV = GridSearchCV(RandomForestClassifier(), param_grid=hyperparameters, cv=10, n_jobs=4)\n",
    "gridCV.fit(XTrain, yTrain)\n",
    "\n",
    "best_n_estim      = gridCV.best_params_['n_estimators']\n",
    "best_max_features = gridCV.best_params_['max_features']               \n",
    "best_max_depth    = gridCV.best_params_['max_depth']\n",
    "\n",
    "# Train classifier using optimal hyperparameter values\n",
    "# We could have also gotten this model out from gridCV.best_estimator_\n",
    "clfRDF = RandomForestClassifier(n_estimators=best_n_estim, max_features=best_max_features, max_depth=best_max_depth)\n",
    "clfRDF.fit(XTrain, yTrain)\n",
    "RF_predictions = clfRDF.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, RF_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, RF_predictions),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93-95% accuracy, not too shabby! Have a look and see how random forests with suboptimal hyperparameters fare. We got around 91-92% accuracy on the out of the box (untuned) random forests, which actually isn't terrible. \n",
    "\n",
    "### 2) Second algorithm: support vector machines\n",
    "\n",
    "Let's train our second algorithm, support vector machines (SVMs) to do the same exact prediction task. A great introduction to the theory behind SVMs can be read [here](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners). Briefly, SVMs search for hyperplanes in the feature space which best divide the different classes in your dataset. Crucially, SVMs can find non-linear decision boundaries between classes using a process called kernelling, which projects the data into a higher-dimensional space. This sounds a bit abstract, but if you've ever fit a linear regression to power-transformed variables (e.g. maybe you used x^2, x^3 as features), you're already familiar with the concept.\n",
    "\n",
    "SVMs can use different types of kernels, like Gaussian or radial ones, to throw the data into a different space. The main hyperparameters we must tune for SVMs are gamma (a kernel parameter, controlling how far we 'throw' the data into the new feature space) and C (which controls the [bias-variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) of the model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.94      0.95      0.94       170\n",
      "        yes       0.88      0.86      0.87        80\n",
      "\n",
      "avg / total       0.92      0.92      0.92       250\n",
      "\n",
      "Overall Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "g_range = 2. ** np.arange(-15, 5, step=2)\n",
    "C_range = 2. ** np.arange(-5, 15, step=2)\n",
    "\n",
    "hyperparameters = [{'gamma': g_range, \n",
    "                    'C': C_range}] \n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(SVC(), param_grid=hyperparameters, cv= 10)  \n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "bestG = grid.best_params_['gamma']\n",
    "bestC = grid.best_params_['C']\n",
    "\n",
    "# Train SVM and output predictions\n",
    "rbfSVM = SVC(kernel='rbf', C=bestC, gamma=bestG)\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "SVM_predictions = rbfSVM.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, SVM_predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, SVM_predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! This is similar performance to what we saw in the random forests.\n",
    "\n",
    "### 3) Third algorithm: neural network\n",
    "\n",
    "Finally, let's jump on the hype wagon and throw neural networks at our problem.\n",
    "\n",
    "Neural networks (NNs) represent a different way of thinking about machine learning algorithms. A great place to start learning about neural networks and deep learning is [this resource](http://neuralnetworksanddeeplearning.com/about.html). Briefly, NNs are composed of  multiple layers of artificial neurons, which individually are simple processing units that weigh up input data. Together, layers of neurons can work together to compute some very complex functions of the data, which in turn can make excellent predictions. You may be aware of some of the crazy results that NN research has recently achieved.\n",
    "\n",
    "Here, we train a shallow, fully-connected, feedforward neural network on the spam dataset. Other types of neural network implementations in scikit are available here. The hyperparameters we optimize here are the overall architecture (number of neurons in each layer and the number of layers) and the learning rate (which controls how quickly the parameters in our network change during the training phase; see [gradient descent](http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent) and [backpropagation](http://neuralnetworksanddeeplearning.com/chap2.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.95      0.92      0.93       170\n",
      "        yes       0.84      0.89      0.86        80\n",
      "\n",
      "avg / total       0.91      0.91      0.91       250\n",
      "\n",
      "Overall Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "from multilayer_perceptron import multilayer_perceptron\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "layer_size_range = [(3,2),(10,10),(2,2,2),10,5] # different networks shapes\n",
    "learning_rate_range = np.linspace(.1,1,3)\n",
    "hyperparameters = [{'hidden_layer_sizes': layer_size_range, 'learning_rate_init': learning_rate_range}]\n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(multilayer_perceptron.MultilayerPerceptronClassifier(), param_grid=hyperparameters, cv=10)\n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "# Output best hyperparameter values\n",
    "best_size    = grid.best_params_['hidden_layer_sizes']\n",
    "best_best_lr = grid.best_params_['learning_rate_init']\n",
    "\n",
    "# Train neural network and output predictions\n",
    "nnet = multilayer_perceptron.MultilayerPerceptronClassifier(hidden_layer_sizes=best_size, learning_rate_init=best_best_lr)\n",
    "nnet.fit(XTrain, yTrain)\n",
    "NN_predictions = nnet.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, NN_predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, NN_predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this neural network (given this dataset, architecture, and hyperparameterisation) is doing slightly worse on the spam dataset. That's okay, it could still be picking up on a signal that the random forest and SVM weren't. \n",
    "\n",
    "Machine learning algorithns... ensemble!\n",
    "\n",
    "### 4) Majority vote on classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority vote for test point 0 is:  1\n",
      "The majority vote for test point 1 is:  0\n",
      "The majority vote for test point 2 is:  0\n",
      "The majority vote for test point 3 is:  0\n",
      "The majority vote for test point 4 is:  0\n",
      "The majority vote for test point 5 is:  1\n",
      "The majority vote for test point 6 is:  1\n",
      "The majority vote for test point 7 is:  0\n",
      "The majority vote for test point 8 is:  1\n",
      "The majority vote for test point 9 is:  1\n",
      "The majority vote for test point 10 is:  0\n",
      "The majority vote for test point 11 is:  0\n",
      "The majority vote for test point 12 is:  0\n",
      "The majority vote for test point 13 is:  0\n",
      "The majority vote for test point 14 is:  1\n",
      "The majority vote for test point 15 is:  0\n",
      "The majority vote for test point 16 is:  0\n",
      "The majority vote for test point 17 is:  0\n",
      "The majority vote for test point 18 is:  0\n",
      "The majority vote for test point 19 is:  0\n",
      "The majority vote for test point 20 is:  0\n",
      "The majority vote for test point 21 is:  0\n",
      "The majority vote for test point 22 is:  1\n",
      "The majority vote for test point 23 is:  1\n",
      "The majority vote for test point 24 is:  0\n",
      "The majority vote for test point 25 is:  0\n",
      "The majority vote for test point 26 is:  0\n",
      "The majority vote for test point 27 is:  1\n",
      "The majority vote for test point 28 is:  0\n",
      "The majority vote for test point 29 is:  0\n",
      "The majority vote for test point 30 is:  0\n",
      "The majority vote for test point 31 is:  0\n",
      "The majority vote for test point 32 is:  1\n",
      "The majority vote for test point 33 is:  0\n",
      "The majority vote for test point 34 is:  0\n",
      "The majority vote for test point 35 is:  0\n",
      "The majority vote for test point 36 is:  0\n",
      "The majority vote for test point 37 is:  0\n",
      "The majority vote for test point 38 is:  0\n",
      "The majority vote for test point 39 is:  0\n",
      "The majority vote for test point 40 is:  1\n",
      "The majority vote for test point 41 is:  0\n",
      "The majority vote for test point 42 is:  0\n",
      "The majority vote for test point 43 is:  1\n",
      "The majority vote for test point 44 is:  0\n",
      "The majority vote for test point 45 is:  0\n",
      "The majority vote for test point 46 is:  0\n",
      "The majority vote for test point 47 is:  0\n",
      "The majority vote for test point 48 is:  1\n",
      "The majority vote for test point 49 is:  0\n",
      "The majority vote for test point 50 is:  0\n",
      "The majority vote for test point 51 is:  1\n",
      "The majority vote for test point 52 is:  0\n",
      "The majority vote for test point 53 is:  0\n",
      "The majority vote for test point 54 is:  0\n",
      "The majority vote for test point 55 is:  0\n",
      "The majority vote for test point 56 is:  0\n",
      "The majority vote for test point 57 is:  0\n",
      "The majority vote for test point 58 is:  0\n",
      "The majority vote for test point 59 is:  1\n",
      "The majority vote for test point 60 is:  0\n",
      "The majority vote for test point 61 is:  0\n",
      "The majority vote for test point 62 is:  1\n",
      "The majority vote for test point 63 is:  0\n",
      "The majority vote for test point 64 is:  1\n",
      "The majority vote for test point 65 is:  1\n",
      "The majority vote for test point 66 is:  0\n",
      "The majority vote for test point 67 is:  1\n",
      "The majority vote for test point 68 is:  1\n",
      "The majority vote for test point 69 is:  1\n",
      "The majority vote for test point 70 is:  0\n",
      "The majority vote for test point 71 is:  1\n",
      "The majority vote for test point 72 is:  0\n",
      "The majority vote for test point 73 is:  0\n",
      "The majority vote for test point 74 is:  0\n",
      "The majority vote for test point 75 is:  0\n",
      "The majority vote for test point 76 is:  0\n",
      "The majority vote for test point 77 is:  1\n",
      "The majority vote for test point 78 is:  0\n",
      "The majority vote for test point 79 is:  0\n",
      "The majority vote for test point 80 is:  0\n",
      "The majority vote for test point 81 is:  1\n",
      "The majority vote for test point 82 is:  1\n",
      "The majority vote for test point 83 is:  0\n",
      "The majority vote for test point 84 is:  1\n",
      "The majority vote for test point 85 is:  1\n",
      "The majority vote for test point 86 is:  0\n",
      "The majority vote for test point 87 is:  0\n",
      "The majority vote for test point 88 is:  0\n",
      "The majority vote for test point 89 is:  0\n",
      "The majority vote for test point 90 is:  1\n",
      "The majority vote for test point 91 is:  0\n",
      "The majority vote for test point 92 is:  1\n",
      "The majority vote for test point 93 is:  0\n",
      "The majority vote for test point 94 is:  0\n",
      "The majority vote for test point 95 is:  0\n",
      "The majority vote for test point 96 is:  0\n",
      "The majority vote for test point 97 is:  1\n",
      "The majority vote for test point 98 is:  1\n",
      "The majority vote for test point 99 is:  0\n",
      "The majority vote for test point 100 is:  1\n",
      "The majority vote for test point 101 is:  0\n",
      "The majority vote for test point 102 is:  1\n",
      "The majority vote for test point 103 is:  0\n",
      "The majority vote for test point 104 is:  0\n",
      "The majority vote for test point 105 is:  0\n",
      "The majority vote for test point 106 is:  1\n",
      "The majority vote for test point 107 is:  0\n",
      "The majority vote for test point 108 is:  0\n",
      "The majority vote for test point 109 is:  0\n",
      "The majority vote for test point 110 is:  0\n",
      "The majority vote for test point 111 is:  1\n",
      "The majority vote for test point 112 is:  0\n",
      "The majority vote for test point 113 is:  0\n",
      "The majority vote for test point 114 is:  0\n",
      "The majority vote for test point 115 is:  0\n",
      "The majority vote for test point 116 is:  0\n",
      "The majority vote for test point 117 is:  0\n",
      "The majority vote for test point 118 is:  0\n",
      "The majority vote for test point 119 is:  0\n",
      "The majority vote for test point 120 is:  1\n",
      "The majority vote for test point 121 is:  0\n",
      "The majority vote for test point 122 is:  1\n",
      "The majority vote for test point 123 is:  0\n",
      "The majority vote for test point 124 is:  0\n",
      "The majority vote for test point 125 is:  1\n",
      "The majority vote for test point 126 is:  0\n",
      "The majority vote for test point 127 is:  1\n",
      "The majority vote for test point 128 is:  0\n",
      "The majority vote for test point 129 is:  1\n",
      "The majority vote for test point 130 is:  0\n",
      "The majority vote for test point 131 is:  0\n",
      "The majority vote for test point 132 is:  1\n",
      "The majority vote for test point 133 is:  0\n",
      "The majority vote for test point 134 is:  1\n",
      "The majority vote for test point 135 is:  0\n",
      "The majority vote for test point 136 is:  0\n",
      "The majority vote for test point 137 is:  1\n",
      "The majority vote for test point 138 is:  1\n",
      "The majority vote for test point 139 is:  0\n",
      "The majority vote for test point 140 is:  1\n",
      "The majority vote for test point 141 is:  0\n",
      "The majority vote for test point 142 is:  0\n",
      "The majority vote for test point 143 is:  1\n",
      "The majority vote for test point 144 is:  0\n",
      "The majority vote for test point 145 is:  1\n",
      "The majority vote for test point 146 is:  1\n",
      "The majority vote for test point 147 is:  1\n",
      "The majority vote for test point 148 is:  0\n",
      "The majority vote for test point 149 is:  0\n",
      "The majority vote for test point 150 is:  1\n",
      "The majority vote for test point 151 is:  0\n",
      "The majority vote for test point 152 is:  0\n",
      "The majority vote for test point 153 is:  0\n",
      "The majority vote for test point 154 is:  0\n",
      "The majority vote for test point 155 is:  0\n",
      "The majority vote for test point 156 is:  0\n",
      "The majority vote for test point 157 is:  0\n",
      "The majority vote for test point 158 is:  0\n",
      "The majority vote for test point 159 is:  1\n",
      "The majority vote for test point 160 is:  1\n",
      "The majority vote for test point 161 is:  0\n",
      "The majority vote for test point 162 is:  0\n",
      "The majority vote for test point 163 is:  0\n",
      "The majority vote for test point 164 is:  1\n",
      "The majority vote for test point 165 is:  0\n",
      "The majority vote for test point 166 is:  1\n",
      "The majority vote for test point 167 is:  0\n",
      "The majority vote for test point 168 is:  0\n",
      "The majority vote for test point 169 is:  1\n",
      "The majority vote for test point 170 is:  1\n",
      "The majority vote for test point 171 is:  0\n",
      "The majority vote for test point 172 is:  0\n",
      "The majority vote for test point 173 is:  0\n",
      "The majority vote for test point 174 is:  0\n",
      "The majority vote for test point 175 is:  0\n",
      "The majority vote for test point 176 is:  0\n",
      "The majority vote for test point 177 is:  0\n",
      "The majority vote for test point 178 is:  1\n",
      "The majority vote for test point 179 is:  0\n",
      "The majority vote for test point 180 is:  0\n",
      "The majority vote for test point 181 is:  1\n",
      "The majority vote for test point 182 is:  0\n",
      "The majority vote for test point 183 is:  0\n",
      "The majority vote for test point 184 is:  0\n",
      "The majority vote for test point 185 is:  1\n",
      "The majority vote for test point 186 is:  1\n",
      "The majority vote for test point 187 is:  1\n",
      "The majority vote for test point 188 is:  1\n",
      "The majority vote for test point 189 is:  0\n",
      "The majority vote for test point 190 is:  0\n",
      "The majority vote for test point 191 is:  0\n",
      "The majority vote for test point 192 is:  0\n",
      "The majority vote for test point 193 is:  0\n",
      "The majority vote for test point 194 is:  0\n",
      "The majority vote for test point 195 is:  0\n",
      "The majority vote for test point 196 is:  0\n",
      "The majority vote for test point 197 is:  0\n",
      "The majority vote for test point 198 is:  1\n",
      "The majority vote for test point 199 is:  0\n",
      "The majority vote for test point 200 is:  0\n",
      "The majority vote for test point 201 is:  0\n",
      "The majority vote for test point 202 is:  1\n",
      "The majority vote for test point 203 is:  0\n",
      "The majority vote for test point 204 is:  0\n",
      "The majority vote for test point 205 is:  0\n",
      "The majority vote for test point 206 is:  0\n",
      "The majority vote for test point 207 is:  0\n",
      "The majority vote for test point 208 is:  0\n",
      "The majority vote for test point 209 is:  0\n",
      "The majority vote for test point 210 is:  0\n",
      "The majority vote for test point 211 is:  1\n",
      "The majority vote for test point 212 is:  0\n",
      "The majority vote for test point 213 is:  0\n",
      "The majority vote for test point 214 is:  0\n",
      "The majority vote for test point 215 is:  1\n",
      "The majority vote for test point 216 is:  1\n",
      "The majority vote for test point 217 is:  0\n",
      "The majority vote for test point 218 is:  0\n",
      "The majority vote for test point 219 is:  0\n",
      "The majority vote for test point 220 is:  0\n",
      "The majority vote for test point 221 is:  1\n",
      "The majority vote for test point 222 is:  0\n",
      "The majority vote for test point 223 is:  0\n",
      "The majority vote for test point 224 is:  0\n",
      "The majority vote for test point 225 is:  0\n",
      "The majority vote for test point 226 is:  0\n",
      "The majority vote for test point 227 is:  0\n",
      "The majority vote for test point 228 is:  0\n",
      "The majority vote for test point 229 is:  1\n",
      "The majority vote for test point 230 is:  0\n",
      "The majority vote for test point 231 is:  1\n",
      "The majority vote for test point 232 is:  0\n",
      "The majority vote for test point 233 is:  1\n",
      "The majority vote for test point 234 is:  0\n",
      "The majority vote for test point 235 is:  1\n",
      "The majority vote for test point 236 is:  0\n",
      "The majority vote for test point 237 is:  1\n",
      "The majority vote for test point 238 is:  0\n",
      "The majority vote for test point 239 is:  0\n",
      "The majority vote for test point 240 is:  0\n",
      "The majority vote for test point 241 is:  1\n",
      "The majority vote for test point 242 is:  0\n",
      "The majority vote for test point 243 is:  1\n",
      "The majority vote for test point 244 is:  0\n",
      "The majority vote for test point 245 is:  0\n",
      "The majority vote for test point 246 is:  1\n",
      "The majority vote for test point 247 is:  1\n",
      "The majority vote for test point 248 is:  1\n",
      "The majority vote for test point 249 is:  0\n"
     ]
    }
   ],
   "source": [
    "# here's a rough solution\n",
    "\n",
    "import collections\n",
    "\n",
    "# stick all predictions into a dataframe\n",
    "predictions = pd.DataFrame(np.array([RF_predictions, SVM_predictions, NN_predictions])).T\n",
    "predictions.columns = ['RF', 'SVM', 'NN']\n",
    "predictions = pd.DataFrame(np.where(predictions=='yes', 1, 0), \n",
    "                           columns=predictions.columns, \n",
    "                           index=predictions.index)\n",
    "\n",
    "# initialise empty array for holding predictions\n",
    "ensembled_predictions = np.zeros(shape=yTest.shape)\n",
    "\n",
    "# majority vote and output final predictions\n",
    "for test_point in range(predictions.shape[0]):\n",
    "    predictions.iloc[test_point,:]\n",
    "    counts = collections.Counter(predictions.iloc[test_point,:])\n",
    "    majority_vote = counts.most_common(1)[0][0]\n",
    "    \n",
    "    # output votes\n",
    "    ensembled_predictions[test_point] = majority_vote.astype(int)\n",
    "    print \"The majority vote for test point\", test_point, \"is: \", majority_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.96      0.95       170\n",
      "          1       0.91      0.89      0.90        80\n",
      "\n",
      "avg / total       0.94      0.94      0.94       250\n",
      "\n",
      "Ensemble Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Get final accuracy of ensembled model\n",
    "yTest[yTest == \"yes\"] = 1\n",
    "yTest[yTest == \"no\"] = 0\n",
    "\n",
    "print metrics.classification_report(yTest.astype(int), ensembled_predictions.astype(int))\n",
    "print \"Ensemble Accuracy:\", round(metrics.accuracy_score(yTest.astype(int), ensembled_predictions.astype(int)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Conclusion\n",
    "\n",
    "There are plenty of ways to do model ensembling. Simple majority voting. We can also do weighted majority voting, where models with higher accuracy get more of a vote. If your output is numerical, you could average. These relatively simple techniques do a great job, but there is more! Stacking (also called blending) is when the predictions from different algorithms are used as input into another algorithm (often good old linear and logistic regression) which then outputs your final predictions. For example, you might train a linear model on the predictions. Blending. \n",
    "\n",
    "\n",
    "\n",
    " It is best to ensemble together models which are less correlated (see an excellent explanation here). \n",
    "See an excellent explanation of ensembling here. \n",
    "\n",
    "\n",
    "What happens when your dataset isn’t as nice as this? What if there are many more instances of one class versus the other, or if you have a lot of missing values, or a mixture of categorical and numerical variables? Stay tuned for the next blog post where we write up guidance on tackling these types of sticky situations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "+ Should we use something cooler like gradient boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best to ensemble together models which are less correlated (see an excellent explanation [here](http://mlwave.com/kaggle-ensembling-guide/)). \n",
    "See an excellent explanation of ensembling [here](http://mlwave.com/kaggle-ensembling-guide/). \n",
    "\n",
    "Another nice tutorial on doing ensembling in python is [here](http://sebastianraschka.com/Articles/2014_ensemble_classifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
