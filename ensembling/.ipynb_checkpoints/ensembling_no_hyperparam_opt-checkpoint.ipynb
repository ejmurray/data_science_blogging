{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective intelligence: combining different machine learning algorithms into an ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model ensembling** is a class of techniques for aggregating together multiple different predictive algorithms into a meta-algorithm, which tends to increase accuracy and reduce overfitting. Ensembling approaches often work surprisingly well. Many winners of competitive data science competitions use model ensembling in [one](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf) [form](https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/5283/winning-solution-code-and-methodology) [or](http://arxiv.org/pdf/0911.0460.pdf) [another](https://medium.com/@chris_bour/6-tricks-i-learned-from-the-otto-kaggle-challenge-a9299378cd61#.utdq3qu1l). In previous tutorials, we discussed how tuning models allows you to get the best performance from individual machine learning algorithms. Here, we will take you through the steps of building your own ensemble for a classification problem, consisting of an individually optimized:\n",
    "\n",
    "1. Random forest (which is already itself an ensemble model)\n",
    "2. Support vector machine and\n",
    "3. Regularized logistic regression classifier\n",
    "\n",
    "These different models have quite different structures, which suggests they might capture different aspects of the dataset and could work well in an ensemble. We’ll continue working on the popular wine dataset, which captures chemical properties of wines and associated wine quality rankings. The goal is to predict wine quality from the chemical properties.  In this post, you'll use the following techniques to build model ensembles: **simple majority voting**, **weighted majority voting**, and model stacking/blending.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The motivation behind ensembling\n",
    "\n",
    "There are also fundamental reasons for why ensembling together different algorithms often improves accuracy, which is extremely well explained in [this Kaggle ensembling guide](http://mlwave.com/kaggle-ensembling-guide/). Briefly, majority voting between models can correct errors in the predictions of individual models. \n",
    "\n",
    "\n",
    "The general idea behind ensembling is this: different classes of algorithms (or differently parameterized versions of the same type of algorithm) might be good at picking up on different signals in the dataset.  Combining them means that you can model the data better, leading to better predictions. Furthermore, different algorithms might be overfitting to the data in various ways, but by combining them, you can effectively average away some of this overfitting. Furthermore, if you're trying to improve your model to chase accuracy points, ensembling is a more computationally effective way to do this than trying to tune a single model by searching for more and more optimal hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "It is best to ensemble together models which are less correlated, because then you can capture different aspects of the blog post (see an excellent explanation [here](http://mlwave.com/kaggle-ensembling-guide/)). \n",
    "See an excellent explanation of ensembling [here](http://mlwave.com/kaggle-ensembling-guide/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of ensemble learning\n",
    "You have probably already encountered several uses of model ensembling. **Random forests** are a type of ensemble algorithm that aggregates together many individual classification tree **base learners**. They are a good systems for intuitively understanding what ensembling is. [Explanation here]. \n",
    "\n",
    "So, a random forest is already an ensemble. But, a random forest will be just one model in the ensemble we build here. 'Ensembling' is a broad term, and is a recurrent concept throughout machine learning, but the general idea is that ensembling can correct the individual parts that may go wrong, and allow different models to capture different signals in the datasetm thereby improving overall performance.\n",
    "\n",
    "If you’re interested in **deep learning**, one common technique for improving classification accuracy is training different neural networks and getting them to vote on classifications for test instances. An ensemble-like technique for training individual neural networks is called dropout, and involves training different subnetworks during the same training phase. Combinging different models is a recurring trend in machine learning, different incarnations. If you’re familiar with **bagging** or **boosting** algorithms, these are very explicit examples of ensembling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this post\n",
    "\n",
    "We will be working on ensembling different algorithms, using both majority voting and stacking,, in order to get improved classification accuracy on the spam dataset. We won’t do fancy visualizations of the dataset, but check out a previous tutorial or our bootcamp to learn Plotly and matplotlib if you're interested. Here, we focus on combining different algorithms to boost performance.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading up the data\n",
    "\n",
    "Load dataset. We often want our input data to be a matrix (X) and the vector of instance labels as a separate vector (y). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Import the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/datasets/wine/winequality-red.csv'\n",
    "dataset = wget.download(data_url)\n",
    "dataset = pd.read_csv(dataset, sep=\";\")\n",
    "\n",
    "# Using a lambda function to bin quality scores\n",
    "dataset['quality_is_high'] = dataset.quality.apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "# Convert the dataframe to a numpy array and split the\n",
    "# data into an input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-2].astype(float)\n",
    "y = npArray[:,-1]\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models\n",
    "Lookin' good! Let's convert the data into a nice format. We rearrange some columns, check out what the columns are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Build rf model\n",
    "best_n_estimators, best_max_features = 73, 5\n",
    "rf = RandomForestClassifier(n_estimators=best_n_estimators, max_features=best_max_features)\n",
    "rf.fit(XTrain, yTrain)\n",
    "rf_predictions = rf.predict(XTest)\n",
    "\n",
    "# Build SVM model\n",
    "best_C_svm, best_gamma = 1.07, 0.01\n",
    "rbf_svm = svm.SVC(kernel='rbf', C=best_C_svm, gamma=best_gamma)\n",
    "rbf_svm.fit(XTrain, yTrain)\n",
    "svm_predictions = rbf_svm.predict(XTest)\n",
    "\n",
    "# Build LR model\n",
    "best_penalty, best_C_lr = \"l2\", 0.52\n",
    "lr = LogisticRegression(penalty=best_penalty, C=best_C_lr)\n",
    "lr.fit(XTrain, yTrain)\n",
    "lr_predictions = lr.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train SVM and output predictions\n",
    "# rbfSVM = svm.SVC(kernel='rbf', C=best_C, gamma=best_gamma)\n",
    "# rbfSVM.fit(XTrain, yTrain)\n",
    "# svm_predictions = rbfSVM.predict(XTest)\n",
    "\n",
    "print (classification_report(yTest, svm_predictions))\n",
    "print (\"Overall Accuracy:\", round(accuracy_score(yTest, svm_predictions),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8386d6cd9980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_C_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_C' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_C, best_C_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Majority vote on classifications\n",
    "\n",
    "We could manually code up a simple implementation of majority voting. It might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# stick all predictions into a dataframe\n",
    "predictions = pd.DataFrame(np.array([rf_predictions, svm_predictions, lr_predictions])).T\n",
    "predictions.columns = ['RF', 'SVM', 'LR']\n",
    "\n",
    "# initialise empty array for holding predictions\n",
    "ensembled_predictions = np.zeros(shape=yTest.shape)\n",
    "\n",
    "# majority vote and output final predictions\n",
    "for test_point in range(predictions.shape[0]):\n",
    "    row = predictions.iloc[test_point,:]\n",
    "    counts = collections.Counter(row)\n",
    "    majority_vote = counts.most_common(1)[0][0]\n",
    "    \n",
    "    # output votes\n",
    "    ensembled_predictions[test_point] = majority_vote.astype(int)\n",
    "    #print \"The majority vote for test point\", test_point, \"is: \", majority_vote\n",
    "\n",
    "print(ensembled_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And we could assess the performance of the majority voted predictions like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get final accuracy of ensembled model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "for individual_predictions in [rf_predictions, svm_predictions, lr_predictions]:\n",
    "#     classification_report(yTest.astype(int), individual_predictions.astype(int))\n",
    "    print \"Accuracy:\", round(accuracy_score(yTest.astype(int), individual_predictions.astype(int)),2)\n",
    "\n",
    "\n",
    "print classification_report(yTest.astype(int), ensembled_predictions.astype(int))\n",
    "print \"Ensemble Accuracy:\", round(accuracy_score(yTest.astype(int), ensembled_predictions.astype(int)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we do not have to do all of this manually, but can use scikit's [`VotingClassifier` class](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named VotingClassifier",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b3e87da76972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from sklearn.ensemble import VotingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVotingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Build and fit majority vote classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ensemble_1 = VotingClassifier(estimators=[('rf', rf), ('svm', rbf_svm), ('lr', lr)], voting='hard')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named VotingClassifier"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "import sklearn.ensemble.VotingClassifier\n",
    "\n",
    "# Build and fit majority vote classifier\n",
    "# ensemble_1 = VotingClassifier(estimators=[('rf', rf), ('svm', rbf_svm), ('lr', lr)], voting='hard')\n",
    "# ensemble_1.fit(XTrain, yTrain)\n",
    "\n",
    "# simple_ensemble_predictions = ensemble_1.predict(XTest)\n",
    "# print metrics.classification_report(yTest, simple_ensemble_predictions)\n",
    "# print \"Ensemble_2 Overall Accuracy:\", round(metrics.accuracy_score(yTest, simple_ensemble_predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a weighted majority vote, where the different base learners are associated with a weight (often reflecting the accuracies of the models, i.e. more accurate models should have a higher weight). These weight the occurence of predicted class labels, which allows certain algorithms to have more of a say in the majority voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting weights\n",
    "\n",
    "ensemble_1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], weights=[1,1,1], voting='hard')\n",
    "ensemble_1.fit(XTrain, yTrain)\n",
    "\n",
    "simple_ensemble_predictions = ensemble_1.predict(XTest)\n",
    "print metrics.classification_report(yTest, simple_ensemble_predictions)\n",
    "print \"Ensemble_2 Overall Accuracy:\", round(metrics.accuracy_score(yTest, simple_ensemble_predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed the `voting='hard'` argument we passed to the VotingClassifier. Setting `voting='soft'` would predict the class labels based on how certain each algorithm in the ensemble was about their individual predictions. This involves calculating the predicted probabilities p for the classifier. Note that scikit only recommends this approach if the classifiers are already tuned well, which should be the case here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], weights=[1,1,1], voting='soft')\n",
    "ensemble_1.fit(XTrain, yTrain)\n",
    "\n",
    "simple_ensemble_predictions = ensemble_1.predict(XTest)\n",
    "print metrics.classification_report(yTest, simple_ensemble_predictions)\n",
    "print \"Ensemble_2 Overall Accuracy:\", round(metrics.accuracy_score(yTest, simple_ensemble_predictions),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Conclusion\n",
    "\n",
    "There are plenty of ways to do model ensembling. Simple majority voting. We can also do weighted majority voting, where models with higher accuracy get more of a vote. If your output is numerical, you could average. These relatively simple techniques do a great job, but there is more! Stacking (also called blending) is when the predictions from different algorithms are used as input into another algorithm (often good old linear and logistic regression) which then outputs your final predictions. For example, you might train a linear model on the predictions. Blending. \n",
    "\n",
    "\n",
    "\n",
    " It is best to ensemble together models which are less correlated (see an excellent explanation here). \n",
    "See an excellent explanation of ensembling here. \n",
    "\n",
    "\n",
    "What happens when your dataset isn’t as nice as this? What if there are many more instances of one class versus the other, or if you have a lot of missing values, or a mixture of categorical and numerical variables? Stay tuned for the next blog post where we write up guidance on tackling these types of sticky situations.\n",
    "\n",
    "\n",
    "Stacking. Combining different techniques. One approach that has been useful in competitive machine learning (where the smallest improvements are crucial to winning). You can add the predictions of different classifiers as additional features. You can then train a variety of models on this new feature set (your old features + these predictions), and average the predictions of these models. Taking the harmonic mean instead of the standard geometric mean. Added the logit of the model’s predictions.\n",
    "Certain types of algorithms tend to do very well in prediction competitions, like Gradient Boosting Trees.XGBoost, a gradient boosting implementation, also popular, R, faster than scikit.\n",
    "\n",
    "Properly parameterised neural networks can be extremely powerful. Though for competitions that are time and computational resource dependent, sometimes not practical.\n",
    "\n",
    "Scikit’s bagging classifier meta-estimator. Run the same algorithm multiple times, random selection of observations and features, take average of output. \n",
    "\n",
    "Probability calibration.\n",
    "\n",
    "We could go a step further with Random Forests - extermely randomized rfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "+ Should we use something cooler like gradient boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Another nice tutorial on doing ensembling in python is [here](http://sebastianraschka.com/Articles/2014_ensemble_classifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
