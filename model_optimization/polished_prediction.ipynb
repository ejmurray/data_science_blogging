{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polished prediction: how to optimize machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When doing machine learning using Python's scikit-learn library, we can often get reasonable model performance by using out-of-the-box algorithms with default values for their settings. However, it is a much better idea to do at least some tuning of the algorithms to your specific problem and dataset. [In the previous post](link.com), we explored the concepts of overfitting, cross-validation, and the bias-variance tradeoff. These ideas turn out to be central to doing a good job at optimizing the settings (or hyperparameters) of algorithms. In this  post, we will explore the concepts behind hyperparamter optimization and demonstrate the process of tuning and training a random forest classifier.\n",
    "\n",
    "You'll be working with the famous (well, machine learning famous!) [spam dataset](https://archive.ics.uci.edu/ml/datasets/Spambase), which contains loads of NLP-mined features of spam and non-spam emails, like the frequencies of the words \"money\", \"free\" and \"viagra\". Our goal is to tune and apply a random forest to these features in order to predict whether a given email is spam. \n",
    "\n",
    "The steps we'll cover in this blog post can be summarized as follows:\n",
    "\n",
    "![Tutorial overview.](hyperparam_intro_rf_only.png)\n",
    "\n",
    "In the next two posts, you will learn about different strategies for model optimization, how to tune a support vector machine and logistic regression classifier, and find out how to take different tuned machine learning algorithms and combine them to build an ensemble model (which is a type of aggregated meta-model that often has higher accuracy and lower overfitting than its constituents).\n",
    "\n",
    "Let's get cracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring the dataset\n",
    "\n",
    "We start off by collecting the dataset. It can be found both [online](https://archive.ics.uci.edu/ml/datasets/Spambase) and (in a slightly nicer form) in our GitHub repository, so we just fetch it via `wget` (note: make sure you first type `pip install wget` into your Terminal since `wget` is not a preinstalled Python library). It will download a copy of the dataset to your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64             0   \n",
       "1            0.21               0.28           0.50             0   \n",
       "2            0.06               0.00           0.71             0   \n",
       "3            0.00               0.00           0.00             0   \n",
       "4            0.00               0.00           0.00             0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail   ...     char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00   ...            0.00        0.000   \n",
       "1             0.00            0.94   ...            0.00        0.132   \n",
       "2             0.64            0.25   ...            0.01        0.143   \n",
       "3             0.31            0.63   ...            0.00        0.137   \n",
       "4             0.31            0.63   ...            0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0            0        0.778        0.000        0.000   \n",
       "1            0        0.372        0.180        0.048   \n",
       "2            0        0.276        0.184        0.010   \n",
       "3            0        0.137        0.000        0.000   \n",
       "4            0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  is_spam  \n",
       "0                       278        1  \n",
       "1                      1028        1  \n",
       "2                      2259        1  \n",
       "3                       191        1  \n",
       "4                       191        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "# Import the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/datasets/spam/spam_dataset.csv'\n",
    "dataset = wget.download(data_url)\n",
    "dataset = pd.read_csv(dataset, sep=\",\")\n",
    "\n",
    "# Take a peak at the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done some initial exploration of the dataset in the [previous post](link.com), so let's just jump straight into the modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first convert the pandas dataframe into a numpy array and isolate the outcome variable you'd like to predict (here, 0 means 'non-spam', 1 means 'spam'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert dataframe to numpy array and split\n",
    "# data into input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-1].astype(float)\n",
    "y = npArray[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, let's split the dataset into a training and test set. The training set will be used to develop and tune our predictive models. The test will be completely left alone until the very end, at which point you'll run your finished models on it. Having a test set will allow you to get a good estimate of how well our models would perform out in the wild on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first going to try to predict spam emails with a random forest classifier. Chapter 8 of the [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) book provides a truly excellent introduction to theory behind random forests. Briefly, random forests build a collection of classification trees, which each try to predict classes by recursively splitting the data on the features (and feature values) that split the classes best. Each tree is trained on bootstrapped data, and each split is only allowed to use a subset of the available variables. So, an element of randomness is introduced when constructing each tree, which means that a variety of different trees are built. The random forest ensembles these base learners together, i.e. it combines these trees into an aggregated model. Often, the individual trees are asked to predict the class of a new data point, and the random forest simply surveys these predictions and accepts the majority position. This often leads to improved accuracy, generalizability, and stability in the predictions.\n",
    "\n",
    "Out of the box, scikit's random forest classifier already performs quite well on the spam dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.98      0.96       701\n",
      "        1.0       0.96      0.90      0.93       450\n",
      "\n",
      "avg / total       0.95      0.95      0.94      1151\n",
      "\n",
      "('Overall Accuracy:', 0.95)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(XTrain, yTrain)\n",
    "\n",
    "rf_predictions = rf.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, rf_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, rf_predictions),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overall accuracy of 0.94-0.96 is extremely good, but keep in mind that this is a heavily idealized dataset. Next up, we are going to learn how to pick the best parameters for the random forest algorithm in order to get better models with (hopefully!) even higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better modelling through hyperparameter optimization\n",
    "\n",
    "We've glossed over what a hyperparameter actually is. Let's explore the topic now. Often, when setting out to train a machine learning algorithm on your dataset of interest, you must first specify a number of arguments or **hyperparameters** (HPs). A hyperparameter is just a variable than influences the performance of your model, but isn't directly tuned during model training. For example, when using the k-nearest neighbours algorithm to do classification (see [these](link2.com) [two](link1.com) previous posts), the value of k (the number of nearest neighbours the model considers) is a hyperparameter that must be supplied in advance. As another example, when building a neural network, the number of layers in the network and the number of neurons per layer are both hyperparameters that must be specified before training commences. By contrast, the weights and biases in a neural network are **parameters** (not hyperparameters) because they *are* explicitly tuned during training. But how can we know what values to set the hyperparameters to in order to get the best performance from our learning algorithms? \n",
    "\n",
    "Actually, scikit-learn generally provides reasonable hyperparameter default values, such that it is possible to quickly build an e.g. kNN classifier by simply typing `clfr = sklearn.neighbors.KNeighborsClassifier()` and then fitting it to your data. [Behind the scenes](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier), we can see the hyperparameter values that the classifier has automatically assumed, such as setting the number of nearest neighbours hyperparameter to 5 (`n_neighbors=5`), giving all datapoints equal importance (`weights=uniform`), and so on. Often, the default hyperparameters values  will do a decent job (as we saw above), so it may be tempting to skip the topic of model tuning completely. However, it is basically always a good idea to do at least some level of hyperparameter optimization, due to the potential for substantial improvements in your learning algorithm's performance.\n",
    "\n",
    "We optimize hyperparameters in exactly the way that you might expect - we try different values and see what works best. However, some care is needed when deciding how exactly we will measure if certain values work well, and which strategy to use to systematically explore\n",
    "hyperparameter space.\n",
    "\n",
    "In a later post, we will introduce model ensembling, in which individual models can be considered 'hyper-hyper parameters' (&trade;; &copy;; &reg;;  patent pending; T-shirts printing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting to tune our random forest\n",
    "\n",
    "In order to build the best possible model that does a good job at describing the underlying trends in a dataset, we need to pick the right HP values. As we mentioned above, HPs are not optimised while a learning algorithm is learning. Hence, we need other strategies to optimise them. The most basic way would just to test different possible values for the HPs and see how the model performs. In a random forest, [some hyperparameters](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) we can optimise are `n_estimators` and `max_features`. `n_estimators` controls the number of trees in the forest - the more the better (with diminishing returns), but more trees comes at the expense of longer training time. `max_features` controls the size of the random selection of features the algorithm is allowed to consider when splitting a node. Larger values help if the individual predictors aren't that great. Smaller values can be helpful if the features in the dataset are all decent and highly correlated.\n",
    "\n",
    "Let's try out some HP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_estimators = np.array([5, 50])\n",
    "max_features  = np.array([10, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually write a small loop to test out how well the different combinations of these potential HP values fare (later, we'll find out better ways to do this): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n",
      "('Overall Accuracy:', 0.94)\n",
      "(5, 40)\n",
      "('Overall Accuracy:', 0.94)\n",
      "(50, 10)\n",
      "('Overall Accuracy:', 0.96)\n",
      "(50, 40)\n",
      "('Overall Accuracy:', 0.95)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# get grid of all possible combinations of hp values\n",
    "hp_combinations = list(itertools.product(n_estimators, max_features))\n",
    "\n",
    "for hp_combo in range(len(hp_combinations)):\n",
    "    \n",
    "    print (hp_combinations[hp_combo])\n",
    "    \n",
    "    # Train and output accuracies\n",
    "    rf = RandomForestClassifier(n_estimators=hp_combinations[hp_combo][0], \n",
    "                                max_features=hp_combinations[hp_combo][1])\n",
    "    rf.fit(XTrain, yTrain)\n",
    "    RF_predictions = rf.predict(XTest)\n",
    "    print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, RF_predictions),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the higher value of `n_estimators` and the lower value of `max_features` did better. However, manually searching for the best HPs in this way is not efficient and could potentially lead to models that perform well on this specific dataset, but do not generalise well to new data, which is what we are actually interested in. This phenomenon of building models that do not generalise well, or that are fitting too closely to the dataset, is called **overfitting**. This is a very important concept in machine learning and it is very much worth to get a better understanding of what it is. Check out our [previous post]() to learn more. \n",
    "\n",
    "Here, we trained different models on the training dataset using manually selected HP values. We then tested on the test dataset. This is not as bad as training a model and evaluating it on the training set, but is still bad - since we repeatedly evaluate on the test dataset, knowledge of the test set can leak into the model bulding phase. We inadvertenly learn something about the test set, and are susceptible to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation for hyperparameter tuning\n",
    "\n",
    "So, we have to be careful not to overfit to our data. But wait, didn't we also say that the test set is not meant to be touched until we are completely done training our model? How are we meant to optimize our hyperparameters then? \n",
    "\n",
    "Enter **k-fold cross-validation**, which is a handy technique for measuring a model's performance using *only* the training set. k-fold CV is a general method (see an explanation [here]()), and is not specific to hyperparameter optimization, but is very useful for that purpose. Say that we want to do e.g. 10-fold cross-validation. The process is as follows: we randomly partition the training set into 10 equal sections. Then, we train an algorithm on 9/10ths (i.e. 9 out of the 10 sections) of that training set. We then evaluate its performance on the remaining 1 section. This gives us some measure of the model's performance (e.g. overall accuracy). We then train the algorithm on a *different* 9/10ths of the training set, and evaluate on the other (different from before) remaining 1 section. We continue the process 10 times, get 10 different measures of model performance, and average these values to get an overall measure of performance. Of course, we could have chosen some number other than 10. To keep on with the example, the process behind 10-fold CV looks like this:\n",
    "\n",
    "![Diagram showing the steps behind 10-fold cross-validation.](./cv.png \"10-fold CV\")\n",
    "\n",
    "We can use k-fold cross validation to optimize HPs. Say we are deciding whether to use 1,3 or 5 nearest-neighbours in our nearest-neighbours classifier. We can start by setting the `n_neighbours` HP in our classifier object to 1, running 10-fold CV, and getting a measurement of the model's performance. Repeating the process with the other HP values will lead to different levels of performance, and we can simply choose the `n_neighbours` value that worked best.\n",
    "\n",
    "In the context of HP optimization, we perform k-fold cross validation together with **grid search** or **randomized search** to get a more robust estimate of the model performance associated with specific HP values. \n",
    "![Performing the grid search.](./gridsearch_cv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "Traditionally and perhaps most intuitively, scanning for HPs is done with the **grid search** (also called parameter sweep). This strategy exhaustively searches through some manually prespecified HP values and reports the best option. It is common to try to optimize multiple HPs simultaneously - grid search tries each combination of HPs in turn, hence the name. \n",
    "\n",
    "The combination of grid search and k-fold cross validation is very popular for finding the models with good performance and generalisability. So, in HP optimisation we are actually trying to do two things: (i) find the best possible combination of HPs that define a model and (ii) making sure that the pick generalises well to new data. In order to address the second concern, CV is often the method of choice. Scikit-learn makes this process very easy and slick, and even supports parallel distributing of the search (via the `n_jobs` argument).  \n",
    "\n",
    "Note that grid search with k-fold CV simply returns the best HP values out of the available options, and is therefore not guaranteed to return a global optimum. It makes sense to choose a diverse collection of possible values that is somewhat centred around an empirically sensible default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 25, 40], 'max_features': [5, 30, 55]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "n_estimators = list(np.arange(10, 50, 15))\n",
    "max_features  = list(np.arange(5, X.shape[1], 25))\n",
    "\n",
    "hyperparameters   = {'n_estimators': n_estimators, \n",
    "                     'max_features': max_features}\n",
    "\n",
    "print (hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing n_estimators value is:  25.0\n",
      "The best performing max_features value is:   5.0\n",
      "[ 0.00465298  0.0037965   0.02269811  0.0007882   0.02829182  0.00888956\n",
      "  0.08415178  0.01610641  0.00840618  0.01280457  0.00589624  0.01214267\n",
      "  0.00419607  0.00194068  0.00202733  0.06427492  0.01393701  0.01096759\n",
      "  0.0371921   0.01140161  0.04864113  0.00220954  0.04622019  0.02650028\n",
      "  0.04555807  0.01460818  0.02558434  0.00368379  0.00166571  0.00271465\n",
      "  0.00223445  0.0005242   0.00387523  0.00081849  0.00282372  0.00402179\n",
      "  0.01364545  0.00067686  0.00367737  0.00191055  0.00108543  0.00432742\n",
      "  0.00133395  0.00182547  0.01150981  0.01565163  0.00024075  0.00061736\n",
      "  0.0055023   0.01412924  0.00327787  0.11419752  0.05132893  0.0036064\n",
      "  0.07140366  0.05102071  0.0427852 ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.97      0.97       701\n",
      "        1.0       0.95      0.94      0.95       450\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1151\n",
      "\n",
      "('Overall Accuracy:', 0.96)\n"
     ]
    }
   ],
   "source": [
    "# Grid search using cross-validation\n",
    "gridCV = GridSearchCV(RandomForestClassifier(), param_grid=hyperparameters, cv=10, n_jobs=4)\n",
    "gridCV.fit(XTrain, yTrain)\n",
    "\n",
    "# Identify optimal hyperparameter values\n",
    "best_n_estim      = gridCV.best_params_['n_estimators']\n",
    "best_max_features = gridCV.best_params_['max_features']  \n",
    "\n",
    "print(\"The best performing n_estimators value is: {:5.1f}\".format(best_n_estim))\n",
    "print(\"The best performing max_features value is: {:5.1f}\".format(best_max_features))\n",
    "\n",
    "# Train classifier using optimal hyperparameter values\n",
    "# We could have also gotten this model out from gridCV.best_estimator_\n",
    "rf = RandomForestClassifier(n_estimators=best_n_estim,\n",
    "                                max_features=best_max_features)\n",
    "\n",
    "rf.fit(XTrain, yTrain)\n",
    "RF_predictions = rf.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, RF_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, RF_predictions),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get ~0.96 accuracy. Note that the toy spam dataset we were working on is unusually straightforward, clean, and easy, and we were getting very high accuracies. It is rare to encounter such simple datasets in real life. In this case, we did not improve much on the (unrealistic) baseline of 0.95, but in real life this would usually have a much larger effect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You tuned your random forest classifier! \n",
    "\n",
    "So, that was an overview of the concepts and practicalities involved when tuning a random forest classifer. We could also choose to tune [various other hyperpramaters](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), like `max_depth` (the maximum depth of a tree, which controls how tall we grow our trees and influences overfitting) and the choice of the purity `criterion` (which are specific formulas for calculating how good or 'pure' the bifurcations we choose are, as judged by how well they separate the classes in our dataset). The two HPs we chose to tune are regarded as the most important. \n",
    "\n",
    "### Quick quiz:\n",
    "\n",
    "1. How do you think that altering the `n_estimators` and `max_depth` HPs would affect the bias and variance of the random forest classifier?\n",
    "\n",
    "2. It is interesting that the random forests perform better with quite low values of `max_features` on this dataset. What do you think this says about the features in our dataset? \n",
    "\n",
    "3. Try `max_features=1`. What does this force the trees in the random forest to do? \n",
    "\n",
    "4. To get more of an intuition of how random forests operate, play around with printing the importance of the features with `print (rf.feature_importances_)` under different conditions and experiment with setting `max_depth=0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this post, we started with the motivation for tuning machine learning algorithms (i.e. nicer, bigger numbers in your models' performance reports!). We used different methods of searching over hyperparameter space, and evaluated candidate models at different points using k-fold cross validation. Tuned models were then run on the test set. Note that the concepts of training/test sets, cross-validation, and overfitting extend beyond the topic of tuning hyperparameters, though it is a nice application to demonstrate these ideas.\n",
    "\n",
    "In this post, we tried to maximize the accuracy of our models, but there are problems for which you might want to maximize something else, like the model's specificity or the sensitivity. For example, if we were doing medical diagnostics and trying to detect a deadly illness, it would be very bad to accidentally label a sick person as healthy (this would be called a \"false negative\" in the lingo). Maybe it's not so bad if we misclassify healthy people as sick people (\"false positive\"), since in the worst case we would just annoy people by having them retake the diagnostic test. Hence, we might want our diagnostic model to be weighted towards optimizing sensitivity. [Here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/) is a good introduction to sensitivity and specificity which continues with the example of diagnostic tests.\n",
    "\n",
    "Arguably, in spam detection, it is worse to misclassify real email as spam (false positive) than to let a few spam emails pass through your filter (false negative) and show up in people's mailboxes. In this case, we might aim to maximize specificity. Of course, we cannot be so focused on improving the specificity of our classifier that we completely bomb our sensitivity. There is a natural trade-off between these quantities (see [this primer on ROC curves](http://www.uphs.upenn.edu/radiology/education/resources/documents/receiver-operator-characteristic-analysis-primer.pdf)), and part of our job as statistical modellers is to practice the dark art of deciding where to draw the line.\n",
    "\n",
    "Sometimes there is no tuning to be done. For example, a Naive Bayes (NB) classifier just operates by calculating conditional probabilities, and there is no real hyperparameter optimization stage. NB is actually a very interesting algorithm that is famous for classifying text documents (and the `spam` dataset in particular), so if you have time, check out a great overview and Python implementation [here]((https://bionicspirit.com/blog/2012/02/09/howto-build-naive-bayes-classifier.html)). It's a \"naive\" classifier because it rests on the assumption that the features in your dataset are independent, which is often not strictly true. In our spam dataset, you can image that the occurence of the strings \"win\", \"money\", and \"!!!\" is probably not independent. Despite this, NB often still does a decent job at classification tasks. \n",
    "\n",
    "In our next post, we will explore different ways to tune models and optimise a support vector machine and logistic regression classifier. Stay... tuned! *Cue groans*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
