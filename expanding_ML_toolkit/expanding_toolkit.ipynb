{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding your machine learning toolkit: randomized search,  SVMs and regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Previously, we wrote about some [common trade-offs](http://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/) in machine learning and the importance of [tuning models](http://blog.cambridgecoding.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/) to your specific dataset. We demonstrated tuning a **random forest** classifier using grid search, and how **cross-validation** can help avoid **overfitting** when tuning **hyperparameters**.\n",
    "\n",
    "In this short follow-up post, we introduce a different strategy for traversing hyperparameter space - **randomized search**. We also demonstrate the process of tuning and training two other classification algorithms - a **support vector machine** and a **logistic regression classifier**.\n",
    "\n",
    "![Algorithms we'll use in this tutorial.](hyperparam_algos_logistic.png)\n",
    "\n",
    "We'll keep working with the [wine dataset](https://archive.ics.uci.edu/ml/datasets/Wine), which contains chemical characteristics of wines of varying quality. As before, our goal is to try to predict a wine's quality form these features.\n",
    "\n",
    "Here are the things we'll cover in this blog post:\n",
    "\n",
    "![Tutorial overview.](hyperparam_intro_logistic.png)\n",
    "\n",
    "In the next blog post, you will learn how to take these three different tuned machine learning algorithms and combine them to build an aggregate **model ensemble**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and train/test splitting the dataset\n",
    "\n",
    "You start off by collecting the dataset. We have covered the data loading, preprocessing conversion and train/test split [previously](http://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/), so we won't repeat ourselves here. Also check out [this post](http://blog.cambridgecoding.com/2016/02/07/eda-and-interactive-figures-with-plotly/) on using plotly to create exploratory, interactive graphics of the wine dataset features. \n",
    "\n",
    "You can fetch and format the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Import the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/datasets/wine/winequality-red.csv'\n",
    "dataset = wget.download(data_url)\n",
    "dataset = pd.read_csv(dataset, sep=\";\")\n",
    "\n",
    "# Using a lambda function to bin quality scores\n",
    "dataset['quality_is_high'] = dataset.quality.apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "# Convert the dataframe to a numpy array and split the\n",
    "# data into an input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-2].astype(float)\n",
    "y = npArray[:,-1]\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing things up: introducing randomized search\n",
    "\n",
    "You have already built a random forest classifier, tuned using grid search, to predict wine quality ([here](http://localhost:8888/notebooks/polished_prediction/scanning_hyperspace.ipynb#)). Grid search exhaustively searches through some manually prespecified HP values and reports the best option and is quite commonly used. Another way to search through hyperparameter space to find optimums is by using **randomized search**. In randomized search, we sample HP values a certain number of times from some distribution which we prespecify in advance. There is [evidence](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) that randomized search is more efficient than grid search, because not all HPs are as important to tune and grid search effectively wastes time by exhaustively checking each option when it might not be necessary. By contrast, the random experiments utilized by randomized search explore the important dimensions of hyperparameter space with more coverage, while simultaneously not devoting too many trials to dimensions which are not as important. So, randomized search is very useful for high-dimensional feature spaces. \n",
    "\n",
    "To use randomized search to tune random forests, we first specify the distributions we want to sample from.\n",
    "\n",
    "If we were to sample from a uniform distribution and have the same number of n_iter trials, randomized search would be practically equivalent to grid search (but not exactly equivalent - why is this?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [40, 36, 21, 12, 44], 'max_features': [7, 8, 4, 5, 9]}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Designate distributions to sample hyperparameters from \n",
    "n_estimators = np.random.uniform(10, 45, 5).astype(int)\n",
    "max_features = np.random.normal(5, 3, 5).astype(int)\n",
    "\n",
    "hyperparameters = {'n_estimators': list(n_estimators),\n",
    "                   'max_features': list(max_features)}\n",
    "\n",
    "print hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing n_estimators value is:  36.0\n",
      "The best performing max_features value is:   4.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.82      0.80       188\n",
      "        1.0       0.83      0.79      0.81       212\n",
      "\n",
      "avg / total       0.81      0.81      0.81       400\n",
      "\n",
      "('Overall Accuracy:', 0.81)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Run randomized search\n",
    "randomCV = RandomizedSearchCV(RandomForestClassifier(), param_distributions=hyperparameters, n_iter=10)\n",
    "randomCV.fit(XTrain, yTrain)\n",
    "\n",
    "# Identify optimal hyperparameter values\n",
    "best_n_estim      = randomCV.best_params_['n_estimators']\n",
    "best_max_features = randomCV.best_params_['max_features']  \n",
    "\n",
    "print(\"The best performing n_estimators value is: {:5.1f}\".format(best_n_estim))\n",
    "print(\"The best performing max_features value is: {:5.1f}\".format(best_max_features))\n",
    "\n",
    "# Train classifier using optimal hyperparameter values\n",
    "# We could have also gotten this model out from randomCV.best_estimator_\n",
    "clfRDF = RandomForestClassifier(n_estimators=best_n_estim,\n",
    "                                max_features=best_max_features)\n",
    "\n",
    "clfRDF.fit(XTrain, yTrain)\n",
    "RF_predictions = clfRDF.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, RF_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, RF_predictions),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either grid search or randomized search is [probably fine](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html) for tuning random forests.\n",
    "\n",
    "Let's look at how to tune our two other predictors. For simplicity, let's revert back to grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a support vector machine model\n",
    "\n",
    "Let's train our second algorithm, **support vector machines** (SVMs) to do the same exact prediction task. A great introduction to the theory behind SVMs can be found in Chapter 9 of the [Introduction to Statistical Learning book](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) or [in this nice blog post](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners). Briefly, SVMs search for **separating hyperplanes** in the feature space which best divide the different classes in your dataset. If you had 2 features, SVMs would search for the best dividing line; if you had 3 features, it would search for the best dividing 2d plane, etc. Crucially, SVMs can construct complex, **non-linear decision boundaries** between classes using a process called **kernelling**, which projects the data into a higher-dimensional space and makes finding a good boundary easier. This sounds a bit abstract, but if you've ever fit a straight line to power-transformed variables (e.g. maybe you used x^2, x^3 as features as part of linear regression), you're already familiar with the concept of creating additional dimensions to facilitate modelling.\n",
    "\n",
    "SVMs can use different types of kernel functions, like linear, polynomial, Gaussian or radial kernels, to throw the data into a different space. Let's use the popular **radial basis kernel**. Of the available [hyperparameters](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), some key radial SVM hyperparameters to tune are:\n",
    "\n",
    "+ `gamma` (a kernel parameter, controlling how far we 'throw' the data into the new feature space) and\n",
    "+ `C` (which controls the 'softness' of the classification boundary margin and hence the [bias-variance tradeoff](http://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/) of the SVM model\n",
    "\n",
    "Let's examine the default settings and define our own: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Default SVC parameters are:', <bound method SVC.get_params of SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)>)\n",
      "[{'C': array([  3.12500000e-02,   1.00000000e+00,   3.20000000e+01,\n",
      "         1.02400000e+03]), 'gamma': array([  3.05175781e-05,   9.76562500e-04,   3.12500000e-02,\n",
      "         1.00000000e+00])}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "default_SVC = svm.SVC()\n",
    "print (\"Default SVC parameters are:\", default_SVC.get_params)\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "g_range = 2. ** np.arange(-15, 5, step=5)\n",
    "C_range = 2. ** np.arange(-5, 15, step=5)\n",
    "\n",
    "hyperparameters = [{'gamma': g_range, \n",
    "                    'C': C_range}] \n",
    "\n",
    "print (hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing gamma value is:   0.0\n",
      "The best performing C value is: 1024.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.76      0.72       188\n",
      "        1.0       0.76      0.69      0.73       212\n",
      "\n",
      "avg / total       0.73      0.72      0.72       400\n",
      "\n",
      "Overall Accuracy: 0.7225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(SVC(), param_grid=hyperparameters, cv=10)  \n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "best_gamma = grid.best_params_['gamma']\n",
    "best_C = grid.best_params_['C']\n",
    "\n",
    "print (\"The best performing gamma value is: {:5.1f}\".format(best_gamma))\n",
    "print (\"The best performing C value is: {:5.1f}\".format(best_C))\n",
    "\n",
    "# Train SVM and output predictions\n",
    "rbfSVM = SVC(kernel='rbf', C=bestC, gamma=bestG)\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "SVM_predictions = rbfSVM.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, SVM_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, SVM_predictions),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this performance compare an untuned SVM? What about an SVM with especially badly tuned hyperparams?\n",
    "\n",
    "Similarly, here is how you can tune an SVM using randomized search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomized search SVM code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a logistic regression classifier\n",
    "\n",
    "The final model you'll tune and apply to predict wine quality is a logistic regression classifier. This is a type of regression model which is used for predicting binary outcomes (like good wine/not good wine).  Logistic regression fits a sigmoidal (S-shaped) curve through the data, but is essentially just a transformed version of linear regression. In fact, a straight line is fit through transformed data, where the x axes remain the same but the dependent variable is the [log odds](https://en.wikipedia.org/wiki/Logit) of data points being one of the two classes. A nice explanation of the theoretical basis for logistic regression can be found here.\n",
    "\n",
    "One topic you will often encounter in machine learning is **regularization**, which is a class of techniques to reduce overfitting. The idea behind regularization is that you often, it is not good enough to just maximize a model's fit to your data. This is liable to overfitting. Regularization techniques try to cut down on overfitting by penalizing models if e.g. they use too many parameters, or assign coefficients or weights that are \"too big\". This means that models have to learn from the data under a series of constraints, which often leads to robust representations of the data. You can read more about regularized regression [here]().\n",
    "\n",
    "You can adjust just how much regularization you want by adjusting **regularization hyperparameters**, and since this is something you might want to do often, scikit-learn comes with some pre-built models that can very efficiently fit data for a range of regulatization hyperparameter values. This is the case for regularized linear regression models like [Lasso regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) and [ridge regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV), which use l1 and l2 penalties, respectively, to shrink the size of the regression coefficients. These classes offer a shortcut to doing cross-validated selection of the regularization hyperparameter.\n",
    "\n",
    "But you can also optimize how much regularization you want ourselves, while at the same time tuning other hyperparameters like the choice of l1 and l2 penalties, in the same manner as you've been doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing penalty is: l2\n",
      "The best performing C value is:   0.7\n"
     ]
    }
   ],
   "source": [
    "# Tuning a regularized logistic regression model with grid search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "C_range = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "hyperparameters = [{'penalty': penalty, \n",
    "                    'C': C_range}] \n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid=hyperparameters, cv=10)  \n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "best_penalty = grid.best_params_['penalty']\n",
    "best_C = grid.best_params_['C']\n",
    "\n",
    "print (\"The best performing penalty is: {}\".format(best_penalty))\n",
    "print (\"The best performing C value is: {:5.1f}\".format(best_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.73      0.73       188\n",
      "        1.0       0.76      0.75      0.76       212\n",
      "\n",
      "avg / total       0.74      0.74      0.74       400\n",
      "\n",
      "Overall Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Train model and output predictions\n",
    "classifier_logistic = LogisticRegression(penalty=best_penalty, C=best_C)\n",
    "classifier_logistic_fit = classifier_logistic.fit(XTrain, yTrain)\n",
    "logistic_predictions = classifier_logistic_fit.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, logistic_predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, logistic_predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can use randomized search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomized search logistic regression code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you've X. Quick final comparison of grid vs. randomized search. The important thing is to do some exploration of HP space using either method.\n",
    "\n",
    "Fancier techniques for hyperparameter optimization include methods based on [gradient descent](http://jmlr.org/proceedings/papers/v37/maclaurin15.pdf), grad student descent (not recommended), and [Bayesian approaches](http://arxiv.org/pdf/1206.2944.pdf) which update prior beliefs about likely values of hyperparameters based on the data (see [Spearmint](https://github.com/JasperSnoek/spearmint) and [hyperopt](http://hyperopt.github.io/hyperopt/)).\n",
    "\n",
    "In our next post, we will take these different tuned models and build them up into an ensemble model to increase our predictive performance even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
